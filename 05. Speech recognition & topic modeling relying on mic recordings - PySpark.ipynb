{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b61deec0",
   "metadata": {},
   "source": [
    "# <center>Speech recognition & topic modeling relying on mic recordings<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10c11bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"recordings\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fade64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_recordings = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(r\"C:\\Users\\Public\\DW\\___PSYCHOGRAPHICS\\mic_recordings\\mic_recordings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efabc79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quering for audio recording file paths\n",
    "mic_recordings.createOrReplaceTempView(\"mic_recordings\")\n",
    "query = spark.sql(\"\"\"SELECT path FROM mic_recordings\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7cdff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the paths from the query results\n",
    "full_audios = query.select('path').rdd.flatMap(lambda x: x).collect()\n",
    "# removing additional backslashes from the paths\n",
    "full_audios = [path.replace('\\\\\\\\', '\\\\') for path in full_audios]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77bc08d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "r = sr.Recognizer() # creating a SpeechRecognizer instance\n",
    "\n",
    "objs = [] # list to store speech recognition objects\n",
    "\n",
    "# making speech recognition objects\n",
    "for audio in full_audios:\n",
    "    wav = sr.AudioFile(audio)\n",
    "    with wav as source:\n",
    "        objs.append(r.record(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8149932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = []\n",
    "\n",
    "# Using Google Speech Recognition to transcribe the speech\n",
    "for i in range(len(objs)):\n",
    "        text_file.append(r.recognize_google(objs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe741033",
   "metadata": {},
   "source": [
    "### Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db8bff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a DataFrame from the text chunks\n",
    "documents = spark.createDataFrame([(document,) for document in text_file], ['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faa0450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization of the text\n",
    "tokenizer = Tokenizer(inputCol='value', outputCol='tokens')\n",
    "tokenized_df = tokenizer.transform(documents)\n",
    "\n",
    "# removing stop words\n",
    "stopwords = StopWordsRemover.loadDefaultStopWords('english')\n",
    "remover = StopWordsRemover(inputCol='tokens', outputCol='filtered_tokens', stopWords=stopwords)\n",
    "filtered_df = remover.transform(tokenized_df)\n",
    "\n",
    "# creaating a vocabulary of unique words\n",
    "cv = CountVectorizer(inputCol='filtered_tokens', outputCol='features')\n",
    "cv_model = cv.fit(filtered_df)\n",
    "vectorized_df = cv_model.transform(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd0bbac",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4571a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 7  # number of topics to extract\n",
    "max_iterations = 10  # maximum number of iterations for model training\n",
    "\n",
    "# creating a LDA model\n",
    "lda = LDA(k=num_topics, maxIter=max_iterations)\n",
    "\n",
    "# fitting the LDA model\n",
    "lda_model = lda.fit(vectorized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db20012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describing the topics\n",
    "topics = lda_model.describeTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffee1b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------------------------+\n",
      "|topic|terms                                                                                                       |\n",
      "+-----+------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[biology, experience, make, act, try, environmental, yes, mistake, time, full]                              |\n",
      "|1    |[drugs, methamphetamine, pathetic, euphoric, question, cocaine, consumers, cannabis, psilocybin, altruistic]|\n",
      "|2    |[know, good, need, time, important, things, well, saying, trust, slept]                                     |\n",
      "|3    |[thing, sexual, performance, play, whether, case, differences, monoamine, experienced, chemically]          |\n",
      "|4    |[experience, say, mind, really, experiences, strongly, body, people, like, environment]                     |\n",
      "|5    |[impact, play, endogenous, eating, worked, controversial, habits, extent, times, contribute]                |\n",
      "|6    |[drug, take, right, enhance, important, ever, number, one, feel, get]                                       |\n",
      "+-----+------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vocabulary of terms\n",
    "vocab = cv_model.vocabulary\n",
    "\n",
    "# defining a UDF to map term indices to terms\n",
    "map_term_indices_udf = udf(lambda term_indices: [vocab[idx] for idx in term_indices], ArrayType(StringType()))\n",
    "\n",
    "# adding a new column with the mapped terms\n",
    "topics_with_terms = topics.withColumn(\"terms\", map_term_indices_udf(topics.termIndices))\n",
    "\n",
    "# showing the topics with their associated terms\n",
    "topics_with_terms.select('topic','terms').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a176c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
